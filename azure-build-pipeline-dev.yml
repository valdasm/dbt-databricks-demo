# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- master

pool:
  vmImage: 'ubuntu-latest'

container: 
  image: python:3.7
  options: -u root

steps:

- task: DownloadSecureFile@1
  name: SettingProfile
  displayName: 'Downloading Profile for Databricks'
  inputs:
    secureFile: 'profiles.yml'

- script: |
    echo Installing $(SettingProfile.secureFilePath) to the ~/.dbt...
    mkdir ~/.dbt
    cp $(SettingProfile.secureFilePath) ~/.dbt/profiles.yml
  displayName: Installing Profile for Databricks

- script: |
    pip install pipenv
    pipenv install dbt 
    pipenv install dbt-spark 
    pipenv install dbt-spark[PyHive] 
    export PATH=$PATH:/home/vsts_azpcontainer/.local/bin
    pipenv dbt compile
    pipenv dbt test
    pipenv dbt run
  displayName: 'Compile, Test, and Run'